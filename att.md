인성

1분 자기소개 (나 자신에 대한 강점을 어필)

```
개발자 직군에 지원한 김만기입니다.
국비지원학원에서 자바 웹 개발자 6개월 과정을 수료하였습니다.
사정이 있어 이전 회사에서 개발보단 AWS, 리눅스에 대해 많이 접할 기회가 있었습니다.
이러한 경럼은
학원에서 언어 공부만한 개발자와 달리 이러 한 경험은 차별화 된 제 강점이라고 생각합니다.
```



사용해본 기술

```
지금까지 진행하였던 프로젝트는 

sprig boot, jpa maven, jquery, tymeleaf, mariaDb로 진행하였으며 
최근 공부하고 있는것은 Batch, Redis, Docker, jenkins 입니다.

```



차별화된 본인의 강점(나는 과연 어떤 부분에서 회사에 이득이 되는 존재인가, 빠르게 업무투입 가능??)

```
제 강점은 스스로 고민하고 답을 찾으려고 노력하는 성격이라고 생각합니다.
처음 개발에 대해 공부를 하였을 때 학원 선생님께서 너무 많은 질문을 한다고 이야기 해주셨습니다. 이러한 이야기를 듣고 들었던 생각은 '나는 스스로 고민하지 않고 물어보는 구나 ' 였습니다. 이러한 생각이 든 후 개발에 대해 공부하는 방식에 대해 많은 변화가 있었습니다. 스스로 고민하고 테스트해보고 다시 찾아보고 이러한 방식으로 공부를 하게되느 개발자적 사고를 기를 수 있었습니다. 이러한 사고는 개발을 할 때 반드시 필요한 요소라고 생각합니다.

```



왜 이 회사에 지원을 하였는가(정보수집, 회사에 대한 몰입)

```
개발자로서 많은 트래픽의 경험과,다른 개발자들과 헙업을 하고싶어 회사에 지원했습니다.
공고를 확인 하던 중 인라이플의 공고가 눈에 들어왔습니다. 광고 플랫폼 회사이기 때문에 많은 트래픽을 경험 할 수 있을 것이라는 생각이 들었고 지원하기 앞서 회사 홈페이지를 둘러보았을 때 많은 개발자들이 근무를 하는 것을 보고 지금까지와는 다른 많은 개발자들과 협업을 할 수 있을 것이라는 생각이 들었습니다.

```



-------------------------
### F 구하기

- 처치자유도 = 독립변수의 개수 - 1
- 오차 자유도 표본개수 - 독립변수의 수 
- MSR = 처치제곱합 / 처치자유도 
- MSE = 처지제곱합 / 오차자유도
- F = MSR / MSE

오차제곱합 2700

독립변수 10

결정계수 0.5

표본의 개수 100

SST = 처치제0곱합 * (1 + 결정계수)

- 처치자유도 =  10 - 1= 9

- 오차자유도 =  100 - 10 = 90
- MSR = 2700 / 9 = 300
- MSE = 2700 / 90 = 30
- F = 300 / 30  = 10

----

### 지도학습 모형

- KNN
  - 분류
- CART
  - 의사결정트리
- ID3
  - 의사결정트리



----

### 비지도 학습 모형 

FP-Tree

- 연관분석 알고리즘



---

### 회귀분석 모형의 검정

- 결정계수가 클 수록 좋다
- F통계량이 클 수록 좋다(MSR/MSE)
- 잔차제곱평균(MSE)이 작을 수록 좋다
- 회귀제곱평균(MSE)이 클 수록 좋다 



---

### 결정계수

- 결정계수는 0 ~ 1사이 값 이다 
- 결정계수는 1에 가까울 수록 유용성이 높다
- 수정된 결정계수의 값은 결졍계수의 값보다 늘 크거나 같다

---

### 지니지수

1 - (O의 확률 /100 )^2 - (세모의 확률 / 100) ^2

10개중 o가 6개 = 6/10   = 0.6

10개중 세모가 4개 = 4 / 10 = 0.4

 1- (0.6)^2 - (0.4)^2

1 - (0.36) - (0.16) = 0.48



---

### 활성함수

- 사인 함수: 불연속이며 최소 -1 최대 1의 함수
- 시그모이드 : 곡선으로 중심값이 0.5 최소 0 최대 1
- 탄젠트 : 곡선으로 중심값이 0 범위 -1~ 1
- 소프트맥스 : 목표값이 다범주일 경우 
- Relu : 입력이 0이하면 0, 0이상이면 값을 그대로 반환



---

### SVM 마진

하드마진

- 초평면을 기준으로 무조건 하나의 클레스에 속해야한다
- 마진 내에 데이터가 존재 할 수 없다
- 이상치에 민감하다
- 매개변수 C가 존재 하지 않는다

소프트마진 

- 초평면을 기준으로 하나의 클레스에 속하지 않아도 된다
- 마진 내 데이터가 존재 할 수 있다
- 하드마진에 비해 이상치에 민감하지 않다.
- 매개변수 C가 존재한다.

---

### 군집기법

DBScan

- 분포의 형태가 기하학적일 때 효과적

KNN 최근접 알고리즘

- 새로운 데이터가 추가 될 시 가장 가까운 노드를 이용하여 군집화
- k개의 클러스터로 묶음

k-mean

- knn과 유사하지만 거리차이의 분산을 최소화

SOM (Self Organizing Maps)

- 고차원의 데이터를 저차원의 뉴런으로 정렬해 지도 처럼 형상화

---

### EM 알고리즘

- 기대값을 추정하는 E단계
- 기댓값을 최대화 하는 M단계



---

### 잔차도 해석 

선형성 가정 만족

- 기울기가 1 혹은 -1에 가까운가?

정규성 가정 만족

-  데이터가 중앙에 몰려있는가?
-  잔차도를 통해 정규성을 판단 할 수 없다

등분산 가정 만족

- 특정한 패턴이 없이 고르게 분포

---

### 결측치 처리방법

평가치 추정법

- 약간의 오차를 감수하면서 맥락적 상정이나 행렬식 자료를 고려하며 원래의 값을 추정

완전 정보 최대 우도법

- 모수적 데이터에서의 밀도 추정방법

보삽법

- 시계열 자료에서 주로 사용되며나 나머지 해의 자료의 평균을 가지고 계산

---

전진선택법

- AIC가 작은 것부터 하나씩 추가

후진제거법

- 상관계수가 작은 것 부터 F검정 실시

단계적 방법

- AIC가 낮아지는 모델을 찾는방법

---

상관관계가 높은 두 변인

- 여러 독립변수간 높은 상관관계로인해 다중 공선성이 있다고 본다
- Ex) 키(CM), 키(fit)는 높은 상관관계로 인해 변수가 늘어났다
- 이러한 많은 변수를 줄이는 방법으로 PCA가 있다
- 이러한 높은 상관관계를 나타내는 것으로 VIF 가 있으며 10 이상의 값은 높은 상관관계를 나타낸다 

---

### 클래스간 데이터의 양 차이

- SMOTE는 정보의 손실 가능성은 없으나 클래스가 겹칠 수 있다
- 무작위로 정상 데이터의 일부만 남기는 방법은 과소포집
- 많은 클래스의 수를 적은 수의 클래스 만큼 남기는 것을 과소포집
- 과적합 문제를 일으킬 수 있는 것을 과대 포집

---

EDA (탐색적 데이터 분석)

- 그래프를 통한 현시성(Representation)
- 저항성 강조(Resistance)
- 잔차계산(Residual)

---

### 왜도

- positive Skew 는 오른쪽으로 꼬리가 길고 왼쪽에 데이터가 많은 형태
- 왜도가 0보다 크다면 positive
- potitive는 최빈값 < 중앙값 < 평균
- 왜도는 확률 변수의 확률분포 비대칭성을 나타내는 지표



---

이산확률분포

- 이항분포
- 다항분포
- 포아송분포
- 초기하분포
- 기하분포

연속확률분포

- 균등분포
- 정규분포
- 표준정규분포
- 감마분포
- 베타분포
- 지수분포
- t분포
- f분포
- 카이제곱분포

---

### 중심의 극한정리

모집단의 표본이 충분히 크다면 무작위 추출을 하여도 모집단을 대표한다(정규분포에 근사한다)

